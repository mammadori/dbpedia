# copy (and re-compress) all .gz files to .bz2 files in background process, write log output to compress.log
for file in $(find -name '*.gz' | sort) ; do echo $file ; gzip -d <$file | bzip2 -c >${file/%.gz/.bz2} ; done &>compress.log &

# pbzip2 is much faster with multiple cores / CPUs
for file in $(find -name '*.gz' | sort) ; do echo $file ; gzip -d <$file | pbzip2 -c >${file/%.gz/.bz2} ; fi ; done &>compress.log &

# Don't overwrite existing non-empty .bz2 files.
for file in $(find -name '*.gz' | sort) ; do copy=${file/%.gz/.bz2} ; if [[ ! -s $copy ]] ; then echo $file ; gzip -d <$file | pbzip2 -c >$copy ; fi ; done &>compress.log &

# Only generate .bz2 file if it doesn't exist or is older than .gz file.
for file in $(find -name '*.gz' | sort) ; do copy=${file/%.gz/.bz2} ; if [[ "$file" -nt "$copy" ]] ; then echo $file ; gzip -d <$file | pbzip2 -c >$copy ; fi ; done &>compress.log &

# Vice versa: 'copy' all .bz2 files whose .gz companion doesn't exist or is empty 
# using pigz (parallel gzip), in background process, write log output to compress.log
for file in $(find -name '*.bz2' | sort) ; do copy=${file/%.bz2/.gz} ; if [[ ! -s "$copy" ]] ; then echo $file ; bzip2 -d <$file | pigz -c >$copy ; fi ; done &>compress.log &

# find out if all pairs of files (in this case, -skos-categories and -skos-categories-redirected) 
# have almost the same number of bytes (and are therefore almost certainly identical except 
# different dates in header and footer)
find -name *-skos-categories*.nt.gz -ls | sort -k 11 | awk '{if (p == 0) p = $7; else { if (p > $7 + 5 || p < $7 - 5) { print p " " $7 " " $11; } p=0; } }'

# differences between .bz2 files are larger, probably because pbzip2 is a bit different, so relative comparison works better
find -name *-skos-categories*.nt.bz2 -ls | sort -k 11 | awk '{if (p == 0) p = $7; else { if ($7 / p < .99 || p / $7 < .99 ) { print p " " $7 " " $11; } p=0; } }'


# sort a triples file by property (field 2)
# -s means 'stable' - without it, other fields are also sorted, which is a waste of time
sort -s -k 2,2 enwiki-20120601-infobox-properties.ttl >enwiki-20120601-infobox-properties.sorted.ttl

# split a triples file (must be sorted by property name) into separate files for each property
# 29 is the length of the string '<http://dbpedia.org/property/'
# 50 is an arbitrary cutoff to avoid overly long file names
awk '{if ($1 != "#") { if ($2 != last) {last = $2; close(file); counter++; len = length($2) - 29; if (len > 50) len = 50; name = substr($2, 29, len); gsub(/\//, "", name); file = "split-by-property/" sprintf("%.6d", counter) "-" name ".ttl" } print >> file } }' enwiki-20120601-infobox-properties.sorted.ttl

# delete all but the latest directory for each language and print how many GB were freed / kept
# assumes that the shell sorts the result of */* alphabetically
# run this script in base dir of all languages!
# WARNING: even if the latest directory for a language is empty or almost empty, all other directories 
# for this language will be deleted! You probably don't want this. I sure didn't... :-(
du -B 1 -s */* | awk '{split($2, arr, "/"); if (arr[2] != "index.html") { if (name == arr[1]) { system("rm -fr " dir); print dir " <<< gone"; gone += size } else if (name != "") { print dir " <<< kept"; kept += size } name = arr[1]; size = $1; dir = $2 } } END {print dir " <<< kept"; print gone / 1024 / 1024 / 1024; print kept / 1024 / 1024 / 1024 }'

# for all .gz files, print packed size in bytes, lines, unpacked size in bytes, store in packed-lines-bytes.gz.txt
for file in $(find -name '*.gz' | sort) ; do stat --printf='%n %s ' $file ; gzip -d <$file | wc -cl ; done >packed-lines-bytes.gz.txt
# for all .gz files, print name without leading './' and trailing '.gz', print lines, bytes, gzip size, store in lines-bytes-packed.txt
for file in $(find -name '*.gz' | sort) ; do stat --printf='%n %s ' $file ; gzip -d <$file | wc -cl ; done | awk '{sub(/^\.\//, "", $1); sub(/.gz$/, "", $1); print $1 " lines:" $3 " bytes:" $4 " gzip:" $2}' >lines-bytes-packed.txt

# rename all files for which a -redirected dataset now exists to -unredirected
find -mindepth 3 -type f | grep -E '(disambiguations|infobox-properties|mappingbased-properties|page-links|persondata|topical-concepts)' | grep -v -E '(specific-mappingbased|redirected)' | awk '{new=$0; sub(/\.\//, "", new); sub(/\./, "-unredirected.", new); system("echo mv " $0 " " new)}'

# create symbolic links for all *.bz2 files from old directory and name structure to new one
find /home/release/wikipedia -maxdepth 3 -name '*.bz2' | grep -v -E '(ontology.owl|pages-articles.xml)' | env LC_ALL=C sort | while read path ; do file=${path##*/} ; lang=${file%%wiki*} ; tail=${file#*-*-} ; name=${tail%%.*} ; suffix=${tail#*.} ; echo $path ; ln -s $path 3.8/$lang/${name//-/_}_$lang.$suffix ; done